% Chapter Template

\chapter{Problem statement} % Main chapter title

\label{Problem statement} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Problem statement}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the problem statement, as well as the topical environment in which the project takes place.


\section{Introduction}

This project is done on behalf of the company Layzapp. Layzapp is specialized in second-screen solutions. It is currently working on a mobile application which brings relevant information to a second screen during a TV show. The Internet is crawled to find relevant information about a specific topic. The outcome of this search is a certain number of web pages. The content of these web pages is made usable by removing irrelevant data, such as navigation elements, advertisement and login pages. Removing the HTML code is not a very hard task. But after this first process of removal there is still a lot of irrelevant content left, for instance descriptions of further articles or advertisements. Removing this part is much more complicated. 

\section{Task}

The task is to develop a test environment i.e. to evaluate the performance of text extraction algorithms. The test environment needs to classify the quality of a text extraction performed by the different algorithms. For this purpose, each algorithm is fed with a collection of web pages. The outcome is then inspected for its quality. Based on the results and the new knowledge, a new algorithm can be implemented. However, this is a optional requirement.

 \section{Text extraction and algorithms}

 This section cover the subject of text extraction and describes the known algorithms JusText and Boilerpipe.

 \subsection{Text extraction}

 Text extraction or content extraction of web pages is a widely discussed field in research. There are several approaches to this field. The two main approaches are page segmentation via visual or DOM features and boilerplate removal. The main drawback of visual page segmentation is that at some point the web page needs to be rendered and processed as image. This task is very time-consuming and many resources are necessary to fulfill it. This project focuses on algorithms which work with DOM features.
 The basic idea of the two following algorithms was first introduced with the Body Text Extraction (BTE) algorithm \cite{Finn01factor}. The assumptions are the relevant part of the web pages source code content is usually a continuous text where the density of HTML tags is lower than in boilerplate content. Hence, by breaking up the source code in single sections and counting the HTML tags per visible word. There will be an area where the number of HTML tags do not increase. One can expect that this is the article text. Unfortunately, BTE's performance is very limited, but it is still used to compare different extraction algorithms to each other. The two algorithms Boilerpipe and JusText improve the performance of BTE.

 \subsection{Boilerpipe}

 The Boilerpipe  algorithm \cite{algo:boilerpipe} is based on the concept described in the paper "Boilerplate Detection using Shallow Text Features" \cite{paper:boilerpipe} by Kohlsch√ºtter et. al. It uses a variate of HTML tags  to divide the HTML document into blocks. Each block is classified according to its shallow text features such as word length, link density etc. The classification uses features of the current, previous and next blocks. The features and the classification algorithm are described more closely in the paper.

 \subsection{JusText}

The algorithm JusText \cite{algo:justext} is based on the concepts described in paper "Removing Boilerplate and Duplicate Content from Web Corpora" \cite{paper:justext} by Pomikalek et. al. It uses similar features as Boilerplate but inspects the data for the presence of stop words as well. Some examples for stop words are 'a', 'and', 'but', 'how', 'or', and 'what'. An article contains more stop words than boilerplate. Based on this information, a better classification can be achieved.

\subsection{Classification}

The main task of the application is to rate the performance of the algorithms. The performance can be defined as how much of relevant text is classified as relevant and how much is classified as boilerplate. In information retrieval, these performances can be described in a confusion matrix as in table 1. 

\begin{table}[!ht]
\begin{tabular}{|p{4cm} |p{5.5cm} |p{5.5cm} |}\hline
          								& \textbf{Classified as content} 	& \textbf{Classified as boilerplate} 	\\ \hline
\textbf{Actual content} 				& True positive (TP)				& False negative(FN)					\\ \hline
\textbf{Actual boilerplate} 			& False positive (FP)       		& True negative (TN)				 	\\ \hline
\end{tabular}
\caption[asdfasdf]{The confusion matrix and its meaning}
\label{confusionMatrix}
\end{table}

\begin{itemize}
\item True positive is an instance which is relevant content and classified as content by the algorithm
\item False positive is an instance which is relevant content but classified as boilerplate by the algorithm 
\item True negative is an instance which is boilerplate and is classified as boilerplate by the algorithm
\item False negative is an instance which is boilerplate but is classified as content by the algorithm
\end{itemize}

These values are the basis for calculating  both the recall (also known as sensitivity) which is the fraction of relevant text that is retrieved and the precision which is the fraction of retrieved text that is relevant. These values, their dependencies and some more values are described more closely in the software requirement specification (\ref{subsec:Evaluation of classification}).

\section{Goals}

\begin{itemize}
\item Implement a content extraction test framework which can be fed with test data and compare multiple text extraction algorithm with each other.
\item Defining a set of representative test data which consists of HTML source files with according text files which define the relevant content of these HTML files.
\item Integrate both the JusText and the Boilerpipe algorithm into the test framework.
\item Evaluate the results of the content extraction test framework with statistical methods and compose a statement of the quality of the tested algorithms.
\item Implement a new algorithm based on results of the test framework and the newly gathered knowledge about text extraction in cooperation with the supervisors. This requirement is optional.
\end{itemize}



\section{Summary}

An application is needed which can be fed with a  collection of web page source code and the corresponding relevant text. These documents are then processed by text extraction algorithms. The outcome is then compared with the relevant text and classification values are calculated. Based on these values the performance of the algorithms can be compared. Possible strengths and weaknesses should be able determined in this comparison.

