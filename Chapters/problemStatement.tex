% Chapter Template

\chapter{Problem statement} % Main chapter title

\label{Problem statement} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Problem statement}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the problem statement as well as the topical environment where the project takes place.


\section{Introduction}

This project is done in behalf of the company Layzapp. Layzapp is specialized on second-screen solutions and they are currently working on a mobile application which brings relevant content to a second screen during a TV show. To do so the Internet is crawled for relevant information about a certain topic. The outcome of this search are a certain amount of web pages. To use the content of these web pages irrelevant data such as navigation elements, advertisement and login pages need to be removed. Removing the HTML content is not a very hard task but after doing so there is still a lot of content left such as description of further articles or advertisement which are not relevant for the user. Removing this part is much more complicated. 

\section{Task}

As described in the introduction, irrelevant content such as advertisement, navigation elements and further articles which are also called boilerplate, need to be removed from a web page. There are already several algorithm which fulfill this task. To compare the performance of the known algorithm and contrast them to possible new algorithms a test environment is needed. The test environment needs to classify the quality of a text extraction performed by the different algorithms. To do so each algorithm is feed with a certain amount if HTML content and the outcome is then inspected for its quality. 

 \section{Text extraction and algorithms}

 This chapter is a short description of the text extraction subject and the known algorithms.

 \subsection{Text extraction}

 Text extraction or content extraction of web pages is a widely discussed field in research. There are several approaches to this field. The two main approaches are page segmentation via visual and DOM features and boilerplate removal. The main drawback of visual page segmentation is that at some point the web page needs to be rendered and processed as image which is a very time and resource consuming task. In this project the focus is on algorithms which work with boilerplate removal.
 The basic idea of boilerplate removal was first introduced with the BTE (Body Text Extraction) algorithm. The assumptions are the relevant part of the HTML content is usually a contiguous stretch, the density of HTML tags is lower in it that in boilerplate content. Hence, by breaking up the HTML page in single sections and counting the HTML tags, there will be an area where the number of HTML tags will not increase. It is quite simple to define an objective function. One can expect that this is the article text. Unfortunately, BTE's performance is very limited, but it is still use to compare different extraction algorithms to each other. The two algorithms Boilerpipe and Justext are improving the performance of BTE and which are used in this project to work with in a first approach.

 \subsection{Boilerpipe}

 The Boilerpipe algorithm is based on the concept described in Kopfsch√ºttels paper "Boilerplate Detection using Shallow Text Features". It uses a variate of HTML tags  to divide the HTML document into blocks. Each block is classified with shallow text features and depending the classification of the  previous and the next blocks. Some examples for shallow text features are average word length, average sentence length or the absolute number of words. The features are described more close in the paper.

 \subsection{Justext}

Justext uses similar features as Boilerplate but is inspecting the presence of stop words as well. Some examples for stop words are "a", "and", "but", "how", "or", and "what". An article contains more stop words than boilerplate and based on this information, a better classification can be achieved.

\subsection{Classification}

The main task of the application is to classify the performance of the algorithms somehow. The performance can be defined as how much of relevant text is classified as relevant and how much is classified as boilerplate. In information retrieval these performance can be described in a confusion matrix.

\begin{table}[h]
\begin{tabular}{|p{4cm} |p{5.5cm} |p{5.5cm} |}\hline
          								& \textbf{Classified as content} 	& \textbf{Classified as boilerplate} 	\\ \hline
\textbf{Actual content} 				& True positive (TP)				& False negative(FN)					\\ \hline
\textbf{Actual boilerplate} 			& False positive (FP)       		& True negative (TN)				 	\\ \hline
\end{tabular}
\end{table}

\begin{itemize}
\item True positive is the amount of text which is relevant content and classified as content by the algorithm
\item False positive is the amount of text which is relevant content but classified as boilerplate by the algorithm 
\item True negative is the amount of text which is boilerplate and is classified as boilerplate by the algorithm
\item False negative is the amount of text which is boilerplate but is classified as content by the algorithm
\end{itemize}

These values are the basis for calculation the recall (also known as sensitivity) which is the fraction of relevant text that is retrieved and precision which is the fraction of retrieved text that is relevant. These values their dependencies and some more values are described more close in the software requirement specification (\ref{subsec:Evaluation of classification}).

\section{Conclusion}

An application is needed which can be feed with HTML documents which then are processed by text extraction algorithms. The outcome is then compared with the relevant content and classification values are calculated. Based on these values the performance of the algorithms can be compared and possible strengths and weaknesses  can be determined.


