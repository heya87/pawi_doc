% Chapter Template

\chapter{Solution development} % Main chapter title

\label{Solution development} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Solution development}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the approach developing the application. It does not cover each and every detail of the project. Only the key problems are discussed. However all the topics are described more closely in the related project documents in the appendices.

\section{Approach}

\subsection{Planning}

The planning of this project was a very straight forward task. Since it is a small one man project and the start and end dates were clearly defined.  However I am working part time for Layzapp and this project was not the only task during the period of time and the project needed to be coordinated somehow with other tasks. My solution was to work about one to two weeks for this project and switch to other tasks for the next one to two weeks. Switching between multiple projects within one week seemed not very effective because one needs always some hours to get back into the topic. \linebreak
A project plan was done for the whole time line. Five milestones and its delivery objects were defined. As we were working with scrum a milestone was defined as sprint and the delivery objects were divided into stories on the start of each sprint. Since it was a one man team no weekly scrum meetings were done. However on the end of each sprint there was a sprint meeting and the delivery objects were presented to the supervisors and the tasks for the next sprint were defined or adjusted. During the project, the tool toggl (\ref{sec:Time tracking}) was used as a time tracker and task manager. The whole planning as well as the time tracking can be found in the abbreviations under Appendix \ref{Planning}.

\subsection{Programming}

TODO: some other introduction, infos/ref about tools etc.

As a first step a data model and a first approach of a software architecture was developed. This first approach was implemented such that the basic functionality of the application was working. With this working version the data model and the software architecture were approved. 
The next step was to eliminate the biggest risk, the integration of the two algorithms JusText and Boilerpipe into the application. Prototypes for each algorithm were programmed. Integrating the Boilerpipe algorithm \cite{algo:boilerpipe}  was an easy task since it is implemented in Java and could be used without the need of any modification. Integrating the JusText algorithm \cite{algo:justext} was a more difficult task since it is implemented in Python. The final solution was to call the python application with system calls from the java application and read the output text file generated by the python program. \linebreak
The second big risk was the approach comparing the text of the actual content and the outcome of the algorithms and find the classification values (True positive, false positive, true negative, false negative). The different approaches are described for the following example.


The text file with the correct content contains following text.


\texttt{I am an interesting text. My content is about advertisement in modern times.}

The extracted text file by an algorithm contains following text.

\texttt{I am an interesting text. And i am advertisement about socks. }

The first approach was counting each word in the content file and check if it is the same amount of words are available in the extracted text file. In doing so words could occur in different places in a text but could still classified as correct. This is presented in the example. The word 'advertisement' in the content file is part of the actual content. In the extracted text file, the sentence 'My content is about advertisement in modern times.' is not found but there is still a text passage classified as content where the word 'advertisement' occurs. As a result, the word advertisement would be classified as correct (True positive) with this first approach because it is counted once in each text. So this approach could not be used.
In a second approach, the comparison was done by not only comparing a single word but as well as the word before and after the word of interest. 
For the example above, for each word, a group of three words is built and this group is then compared with the second text. The word "advertisement" would be defined as the word group 'about advertisement in' and the second text is searched for this pattern. Since there is not such a group, the word advertisement would be classified wrong (False positive) which would be correct. With this approach the results were already a lot better. But there were still situations where this approach was not working. Instead of implementing an elaborated text comparison algorithm we decided to fall back on existing solutions. Merging tools like diff \cite{wiki:diff} or meld \cite{wiki:meld} are doing exactly the task we were looking for. We decided to use the open source implementation merge-diff-patch \cite{google:diffMatchPatch} developed by google.  The outcome of the comparison are all the words of the input text grouped in 'EQUAL', 'DELETE' and 'INSERT' which can be easy transferred into the needed values True Positive ('EQUAL'), False Positive ('INSERT'), True Negative ('DELETE') and False Negative (remaining words). 


The next task after handling these risks was to integrate the prototypes into the main application. The remaining programming tasks were then refactoring the first approach and testing the application. The final software architecture is described in the abbreviation under Appendix \ref{architecture}. The test concept is described under Appendix \ref{Test plan}. 

The final programming task was then adapting the application so that it is possible to evaluate a single test instead of getting the results for a summary of tests. The reason doing this is described in the section \ref{subsec:Detailed Analysis}. 
The most information about a single test such as Precision, Recall and the extracted text by the algorithms was already available. The more challenging part was the output of the evaluation based on the bocks generated by the algorithms. The problem is that both algorithm have no way of getting the classification of the single blocks. Which means that the two algorithms needed to be modified.

\subsection{Modification JusText}
\label{subsec:Modification JusText}

Even with no prior knowledge at the programming language python, the task was easier then expected. The algorithm is working with blocks during the whole process and prints the blocks classified as content. All we needed to do is to print the classification values at the beginning of each block and parse the output string accordingly with the main application.
This is an example of the printed string after my modification. \linebreak

\begin{lstlisting}
<p class="bad" cfclass="bad" heading="0" word\_count="3" link\_density="1" 
stopword\_count="1" stopword\_density="0"\> Kites with Antennas>
\end{lstlisting}

This string could then be parsed by the Java application into block objects and all the classification data like 'link density' and 'stop word count' could be set for each block.


\subsection{Modification Boilerpipe}

Adapting the boilerpipe algorithm on the other hand was not as easy as expected. Boilerpipe does merge the single blocks during the algorithm and the classification information for some blocks are lost due to this approach. My solution was then to edit the algorithm that each block is backed before it is merged with others and all the available classification information as well. The problem with this approach is that some blocks are classified different at a later point of time and the backed data is not correct anymore. To solve this problem, bigger changes of the algorithm would be needed. However, this solution is good enough to evaluate the results accordingly.


\section{Results}


\subsection{Statistical data}

With the test framework it was finally possible to produce the classification data for the  algorithms using an HTML source file and a content file as input data. Doing this with a few test files, it is not possible to produce a concrete quality criterion for an algorithm. A bigger test data was needed. A gold standard test data was used for this purpose. This test data was used for a text extraction competition called CleanEval \cite{CleanEval:online}. 
The following table shows the overall results when extracting the CeanEval test files with JusText and Boilerpipe and process the results with the test framework.


\begin{tabular}{| p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} |}
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall}  & \textbf{Fallout}  & \textbf{F-Measure}  &\textbf{Accuracy} \\ \hline
    JusText     & 95.29 \%       &  91.99 \%  & 35.03 \% & 91.37 \% & 85.11 \%   \\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%  & 49.31 \% & 79.56 \% &  68.17 \% \\ \hline

\end{tabular}

The meaning of the single result types is described in Appendix section \ref{subsec:Evaluation of classification}.



\subsection{Analysis}

This test data was not only used in this but as well in other papers which broach the issue of text extraction. The paper 'More Effective Boilerplate Removalâ€”the GoldMiner Algorithm' \cite{paper:goldminer} used the same test data to compare their Goldminer algorithm to JusText and Boilerpipe. The results from this paper were used to determine if my approach is heading into the right direction or if the outcome is completely wrong. 
The results from the paper testing the different algorithms with the gold standard  are shown in table \ref{table:resultsGoldminer}.

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    JusText     & 95.29 \%       &  91.99 \%		\\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%		\\ \hline
\end{tabular}
\caption[asdfasdf]{The results for the CleanEval test set from the Goldminer paper}
\label{table:resultsGoldminer}
\end{table}

The results from my test framework are shown in table \ref{table:resultsComparison} 

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    JusText     & 95.86 \%       &  87.27 \%		\\ \hline
    Boilerpipe & 91.14 \%       &  70.60 \%		\\ \hline
\end{tabular}
\caption{The results for the CleanEval test set from the test framework}
\label{table:resultsComparison}
\end{table}


The results are not exaclty the same but they are close. The difference could be explained by several points. First, there is no guaranty that the results from the paper are correct. Second, most of the common approaches comparing these text extraction algorithms were done in comparing the correct classified HTML blocks and not the single words like I did it in this project. This can change the results significantly. Following example should clarify this statement.

Suppose we have an HTML document with ten blocks which have a certain amount of words and are classified by an algorithm as defined in table \ref{blockWordExample}.

\begin{table}[!ht]
\begin{tabular}{| p{2cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Block No.}      & \textbf{Word count}  & \textbf{Classification} 				\\ \hline
    1     & 10      	&  True Positive	\\ \hline
    2     & 100     	&  True Positive	\\ \hline
    3     & 50       	&  True positive	\\ \hline
    4     & 30       	&  True Positive	\\ \hline
    5     & 1       	&  False Positive	\\ \hline
    6     & 1000      	&  False Positive	\\ \hline
    7     & 20      	&  False Positive	\\ \hline
    8     & 300       	&  False Positive	\\ \hline
    9     & 200       	&  False Negative	\\ \hline
    10    & 50        	&  False Negative	\\ \hline
\end{tabular}
\caption{Example Blocks vs. Words}
\label{blockWordExample}
\end{table}

The formulas for calculating Precision and Recall are described in the appendix section \ref{subsec:Evaluation of classification}.
The calculated values for Precision and Recall from table \ref{blockWordExample} is shown in table \ref{ResultsBlockWordExample}

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \ \textbf{Algorithm}  & \textbf{Calculation based on blocks}  & \textbf{Calculation based on words} 				\\ \hline
    Precision     & 50 \%    	&  12.57 \%	\\ \hline
    Recall & 66.67 \%    	&  25.68 \%	\\ \hline
\end{tabular}
\caption{Results example Blocks vs. Words}
\label{ResultsBlockWordExample}
\end{table}

We can see that the difference between the two approach is quite significant because a block which is classified wrong and contains a lot of words is not weighted as high if the values are calculated based on blocks instead of words. 
Comparing algorithms based on words seems to be the better approach since the results are more accurate. Furthermore, each algorithm can define the size of a block by itself and the two algorithms do not produce the same amount of blocks. Comparing a different amount of blocks does not produce very accurate results as well.  

\subsection{Detailed analysis}
\label{subsec:Detailed Analysis}


On the MS4 meeting ( Appendix \ref{ms4report}), we decided not to implement an additional text extraction algorithm because we needed to investigate the problems of the existing algorithms first so that we have an idea what we need to improve for our own algorithm. So instead of focusing on a new approach, we decided that we are improving the functions of the test framework in a way that it is possible to investigate the results of a single test case more closely. These adjustments are described in this section.

As described in the introduction, the algorithms split the HTML file into blocks and classify these blocks based on several classification data. To investigate, why a certain classification went very bad, it would be helpful to get the information about the blocks classification. After modifying the implementation of the algorithms (\ref{subsec:Modification JusText}) it is possible to get this data for both JusText and Boilerpipe. This example shows the results for one block for both JusText and Boilerpipe.


\textbf{Boilerpipe}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 
NOT_DEFINED; text_Density: 3.0; context_Free_classification: NOT_DEFINED; ]
Kites with Antennas
\end{lstlisting}


\textbf{JusText}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 1; 
text_Density: NOT_DEFINED; context_Free_classification: CFC_BAD; ] 
Kites with Antennas
\end{lstlisting}

The classification values which can be extracted from the algorithms are:

\begin{itemize}
\item Link density,
\item Classification,
\item Word count,
\item Stop word count (only JusText),
\item Text density (Only Boilerpipe),
\item Context free classification (Only justext).
\end{itemize}

Having these values for a certain block already helps evaluating how the algorithms work and why a specific block is classified correct or wrong. Some classification values are only used by one algorithm. These values are marked as 'NOT\_DEFINED' in the output file for the according algorithm.
