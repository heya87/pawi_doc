i% Chapter Template

\chapter{Solution development} % Main chapter title

\label{Solution development} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Solution development}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the approach developing the application. It does not cover each and every detail of the project. Only the key problems are discussed. However, all the topics are described more closely in the related project documents in the appendices.

\section{Approach}

\subsection{Planning}

The planning of this project was a very straight forward task. Since it is a small one man project and the start and end dates were clearly defined.  However I am working part time for Layzapp and this project was not my only task during the period of time and the project needed to be coordinated with other tasks. My solution was to work about one to two weeks for this project and switch to other tasks for the next one to two weeks. Switching between multiple projects within one week seemed not very effective because one always needs some hours to get back into a topic. \linebreak
A project plan was done for the whole time line. Five milestones and their delivery objects were defined. As we were working with scrum a milestone was defined as sprint and the delivery objects were divided into stories on the start of each sprint. Since it was a one man team no weekly scrum meetings were done. There was a sprint meeting at the end of each sprint and the delivery objects were presented to the supervisors and the tasks for the next sprint were redefined. During the project, the tool toggl (\ref{sec:Time tracking}) was used as a time tracker and task manager. The whole planning as well as the time tracking can be found in appendix \ref{Planning}.

\subsection{Programming}

As a first step the development environment was set up. The used tools were

\begin{itemize}
\item IDE: Eclipse
\item Version control: Git hosted on GitHub
\item CI server: Travis CI
\item Build tool: Gradle
\end{itemize}

These tools are described in more detail in appendix \ref{architecture:developmentView}. 

As soon as the whole continuous integration process was working, a data model and a first approach of the software architecture was developed. This first approach was implemented so that the basic functionality of the application was working. With this working version the data model and the software architecture could be tested and approved.

The next step was to eliminate the biggest risk, the integration of the two algorithms JusText and Boilerpipe into the application. Prototypes for the algorithms were programmed. Integrating the Boilerpipe algorithm \cite{algo:boilerpipe}  was easy since it is implemented in Java and could be used without the need of any modification. Integrating the JusText algorithm \cite{algo:justext} was a more difficult because it is implemented in Python. The final solution was to call the python application with system calls from the java application and read the output text file generated by the python program.

The second big risk was the approach comparing the text of the actual content with the outcome of the algorithms and find the classification values (TP, TN, FP, FN). The different approaches are described for the following example.


The text file with the expected test results contains following text.


\texttt{I am an interesting text. My content is about advertisement in modern times.}

The extracted text file by an algorithm contains following text.

\texttt{I am an interesting text. And i am an advertisement for socks. }

The first approach was counting each word in the content file and checking if the same amount of words are available in the extracted text file. In doing so words could occur in different places in a text but could still be classified as correct. This is presented in the example. The word 'advertisement' in the content file is part of the actual content. In the extracted text file, the sentence 'My content is about advertisement in modern times.' is not found but there is still a text passage classified as content where the word 'advertisement' occurs. As a result, the word advertisement would be classified as correct (True positive) with this first approach because it is counted once in each text. So this approach could not be used.
In a second approach, the comparison was done by not only comparing a single word but as well as the word before and after the word of interest. 
For the example above, for each word, a group of three words is built and this group is then compared with the second text. The word 'advertisement' would be defined as the word group 'about advertisement in' and the second text is searched for this pattern. Since there is no such group, the word advertisement would be classified wrong (False positive) which would be the correct classification. With this approach the results were already a lot better. But there were still situations where this approach was not working. Instead of implementing an elaborated text comparison algorithm we decided to fall back on existing solutions. Text file comparison tools like diff \cite{wiki:diff} or meld \cite{wiki:meld} are doing exactly the task we were looking for. We decided to use the open source implementation merge-diff-patch \cite{google:diffMatchPatch} developed by google.  The outcome of the comparison performed by diff-match-patch are all the words of the input text grouped in 'EQUAL', 'DELETE' and 'INSERT' which can be easy transferred into the needed values True Positive ('EQUAL'), False Positive ('INSERT'), True Negative ('DELETE') and False Negative (remaining words). 


The next task after handling these risks was to integrate the prototypes into the main application. The remaining programming tasks were then refactoring the first approach and testing the application. The final software architecture is described in the abbreviation under Appendix \ref{architecture}. The test concept is described under Appendix \ref{Test plan}. 

The final programming task would have been the implementation of a new text extraction algorithm. On the MS4 meeting ( Appendix \ref{ms4report}), we decided not to implement an additional text extraction algorithm because we needed to investigate the problems of the existing algorithms first so that we have a better idea what we need to improve for our own algorithm. So instead of focusing on a new approach, we decided to focus improving the test framework. 
The improvement focus on the following two problems. It was not possible to filter test cases of interest from the whole test set and it was not possible to get specific information about one specific test case.

\subsection{Filtering results}
The first needed improvement was the possibility of filtering all the test cases for a certain criterion. To do so, a new configuration item was defined which could be added to the configuration file before starting the test run. The filter function is explained with the following example.

If one wants to have all test cases were JusText had a good rating for precision and Boilerplate did not, the following lines could be added to the configuration file.

\begin{lstlisting}
filter:justext:precision:min:0.8;
filter:boilerpipe:precision:max:0.2;
\end{lstlisting} 

Every test case which has precision value below 80\% for JusText and a precision value above 20\% for Boilerpipe was now removed from the results. The resulting test cases were all test cases where JusText performed quite well and Boilerpipe did not. These test cases can then be investigated more closely.
With this new function it became much easier to investigate certain test cases in more detail.

\subsection{Evaluation based on blocks}

The second needed improvement was the possibility to evaluate a single test instead of all tests.
The most information about a single test such as Precision, Recall and the extracted text by the algorithms was already available. The more challenging part was to get the classification based on the blocks generated by the algorithms. Neither Justext nor Boilerpipe have an API call to get the output results including the classification information for the individual blocks. Which means that the two algorithms needed to be modified.

\subsubsection{Modifications of JusText}
\label{subsec:Modification JusText}

Even with no prior knowledge at the programming language python, the task was easier then expected. The algorithm is working with blocks during the whole process and prints the blocks classified as content. All we needed to do is to print the classification values at the beginning of each block and parse the output string accordingly with the main application.
This is an example of the printed string after my modification. \linebreak

\begin{lstlisting}
<p class="bad" cfclass="bad" heading="0" word_count="3" link_density="1" 
stopword_count="1" stopword_density="0"> Kites with Antennas
\end{lstlisting}

First, all the classification data like 'link density' and 'stop word count' was printed followd by the text of the actual block. Which is 'Kites with Antennas' in the example above. This string can then be parsed by the test framework for further use.


\subsubsection{Modifications to Boilerpipe}

Adapting the Boilerpipe algorithm on the other hand was not as easy as expected. Boilerpipe does merge the single blocks while the algorithm is being carried out and the classification information for some blocks is lost due to this approach. My solution was to change the algorithm so that each block is backed up before it is merged with other blocks with all its available classification information. The problem with this approach is that some blocks are re-classified differently at a later point in time and the backed up data is not correct anymore. Bigger changes of the algorithm would be needed to solve this problem but this would go beyond the scope of this project. Nevertheless, the initial classification can be extracted and this already helps to find possible problems and as well compare it with the results from other algorithms.


\section{Results}


\subsection{Statistical data}

With the test framework it was finally possible to produce the classification data for the  algorithms using an HTML source file and a content file as input data. Doing this with a few test files, it is not possible to produce a concrete quality criterion for an algorithm. A bigger test data set was needed. In the context of a text extraction competition called CleanEval \cite{CleanEval:online}, a large large data set was gathered for their needs. This data set meets the demands of this project in every respect. We decided to use this data set instead of gathering our own data set at the MS1 meeting (\ref{reports:ms1report}). 
The following table shows the overall results when extracting the CeanEval test files with JusText and Boilerpipe and process the results with the test framework.


\begin{tabular}{| p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} |}
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall}  & \textbf{Fallout}  & \textbf{F-Measure}  &\textbf{Accuracy} \\ \hline
    JusText     & 95.29 \%       &  91.99 \%  & 35.03 \% & 91.37 \% & 85.11 \%   \\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%  & 49.31 \% & 79.56 \% &  68.17 \% \\ \hline

\end{tabular}

The meaning of the single result types is described in Appendix section \ref{subsec:Evaluation of classification}.



\subsection{Analysis}

This test data was not only used in this but also in other papers, which broach the issue of text extraction. The paper 'More Effective Boilerplate Removal—the GoldMiner Algorithm' \cite{paper:goldminer} used the same test data to compare their Goldminer algorithm to JusText and Boilerpipe. The results from this paper were used to verify my approach is heading into the right direction or if the outcome is completely wrong. 
The results from the paper testing the different algorithms with the gold standard  are shown in table \ref{table:resultsGoldminer}.

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    JusText     & 95.29 \%       &  91.99 \%		\\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%		\\ \hline
\end{tabular}
\caption[asdfasdf]{The results for the CleanEval test set from the Goldminer paper}
\label{table:resultsGoldminer}
\end{table}

The results from my test framework are shown in table \ref{table:resultsComparison} 

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    JusText     & 95.86 \%       &  87.27 \%		\\ \hline
    Boilerpipe & 91.14 \%       &  70.60 \%		\\ \hline
\end{tabular}
\caption{The results for the CleanEval test set from the test framework}
\label{table:resultsComparison}
\end{table}


The results are not exaclty the same but they are close. The difference could be explained by several points. First, there is no guarantee that the results from the Goldminer paper are correct. Second, most of the common approaches comparing these text extraction algorithms were done in comparing the correct classified HTML blocks and not the single words like I did it in this project. This can change the results significantly. Following example should clarify this statement.

Suppose we have an HTML document with ten blocks which have a certain amount of words and are classified by an algorithm as defined in table \ref{blockWordExample}.

\begin{table}[!ht]
\begin{tabular}{| p{2cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Block No.}      & \textbf{Word count}  & \textbf{Classification} 				\\ \hline
    1     & 10      	&  True Positive	\\ \hline
    2     & 100     	&  True Positive	\\ \hline
    3     & 50       	&  True positive	\\ \hline
    4     & 30       	&  True Positive	\\ \hline
    5     & 1       	&  False Positive	\\ \hline
    6     & 1000      	&  False Positive	\\ \hline
    7     & 20      	&  False Positive	\\ \hline
    8     & 300       	&  False Positive	\\ \hline
    9     & 200       	&  False Negative	\\ \hline
    10    & 50        	&  False Negative	\\ \hline
\end{tabular}
\caption{Example Blocks vs. Words}
\label{blockWordExample}
\end{table}

The formulas for calculating precision and recall are described in the appendix section \ref{subsec:Evaluation of classification}.
The calculated values for precision and recall from table \ref{blockWordExample} is shown in table \ref{ResultsBlockWordExample}

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \ \textbf{Algorithm}  & \textbf{Calculation based on blocks}  & \textbf{Calculation based on words} 				\\ \hline
    Precision     & 50 \%    	&  12.57 \%	\\ \hline
    Recall & 66.67 \%    	&  25.68 \%	\\ \hline
\end{tabular}
\caption{Results example Blocks vs. Words}
\label{ResultsBlockWordExample}
\end{table}

We can see that the difference between the two approaches is quite significant because a large block which is classified wrongs is not weighted as high as if the values are calculated based on blocks instead of words. 
Comparing algorithms based on words seems to be the better approach since the results are more accurate. Furthermore, each algorithm can define the size of a block by itself and the two algorithms do not have to produce the same amount of blocks. Comparing different numbers of blocks does not produce very accurate results either.

\subsection{Detailed analysis}
\label{subsec:Detailed Analysis}

Beside getting these statistical results and comparing the general performance of text extraction algorithms, it is possible to investigate single test cases more closely. To do so, a single line with the test case name needs to be added to the configuration file.

\begin{lstlisting}
inspect:testOfInterest;
\end{lstlisting}

Now the test case 'testOfInterest' is inspected more closely and an extra output file is generated when a test run is performed. Appendix \ref{example:resultsSingleTest} shows an example such a result file.

In the first part, the statistical values are printed followed by the expected content text. Then the results of the tested algorithms are printed, first as continuous text followed by the results formatted as blocks with the according block classification.
The following example shows a single block classified by the algorithms.


\textbf{Boilerpipe}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 
NOT_DEFINED; text_Density: 3.0; context_Free_classification: NOT_DEFINED; ]
Kites with Antennas
\end{lstlisting}


\textbf{JusText}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 1; 
text_Density: NOT_DEFINED; context_Free_classification: CFC_BAD; ] 
Kites with Antennas
\end{lstlisting}

The classification values which can be extracted from the algorithms are:

\begin{itemize}
\item Link density,
\item Classification,
\item Word count,
\item Stop word count (only JusText),
\item Text density (Only Boilerpipe),
\item Context free classification (Only justext).
\end{itemize}

Having these values for a certain block already helps evaluating how the algorithms work and why a specific block is classified correct or wrong. Some classification values are only used by one algorithm. These values are marked as 'NOT\_DEFINED' in the output file for the according algorithm.
