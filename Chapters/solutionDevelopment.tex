i% Chapter Template

\chapter{Solution development} % Main chapter title

\label{Solution development} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Solution development}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the approach developing the application. It does not cover each and every detail of the project. Only the key problems are discussed. However all the topics are described more closely in the related project documents in the appendices.

\section{Approach}

\subsection{Planning}

The planning of this project was a very straight forward task. Since it is a small one man project and the start and end dates were clearly defined.  However I am working part time for Layzapp and this project was not the only task during the period of time and the project needed to be coordinated somehow with other tasks. My solution was to work about one to two weeks for this project and switch to other tasks for the next one to two weeks. Switching between multiple projects within one week seemed not very effective because one needs always some hours to get back into the topic. \linebreak
A project plan was done for the whole time line. Five milestones and its delivery objects were defined. As we were working with scrum a milestone was defined as sprint and the delivery objects were divided into stories on the start of each sprint. Since it was a one man team no weekly scrum meetings were done. However on the end of each sprint there was a sprint meeting and the delivery objects were presented to the supervisors and the tasks for the next sprint were defined or adjusted. During the project, the tool toggl (\ref{sec:Time tracking}) was used as a time tracker and task manager. The whole planning as well as the time tracking can be found in the abbreviations under Appendix \ref{Planning}.

\subsection{Programming}

As a first step the development environment was set up. The used tools were

\begin{itemize}
\item IDE: Eclipse, 
\item Version control: Git, 
\item CI server: Travis CI,
\item Build tool: Gradle.
\end{itemize}

These tools are described in more detail in appendix \ref{architecture:developmentView}. 

As soon as the whole continuous integration process was working, a data model and a first approach of a software architecture was developed. This first approach was implemented so that the basic functionality of the application was working. With this working version the data model and the software architecture could be tested and approved.
The next step was to eliminate the biggest risk, the integration of the two algorithms JusText and Boilerpipe into the application. Prototypes for each algorithm were programmed. Integrating the Boilerpipe algorithm \cite{algo:boilerpipe}  was an easy task since it is implemented in Java and could be used without the need of any modification. Integrating the JusText algorithm \cite{algo:justext} was a more difficult task since it is implemented in Python. The final solution was to call the python application with system calls from the java application and read the output text file generated by the python program. \linebreak
The second big risk was the approach comparing the text of the actual content and the outcome of the algorithms and find the classification values (True positive, false positive, true negative, false negative). The different approaches are described for the following example.


The text file with the correct content contains following text.


\texttt{I am an interesting text. My content is about advertisement in modern times.}

The extracted text file by an algorithm contains following text.

\texttt{I am an interesting text. And i am advertisement about socks. }

The first approach was counting each word in the content file and check if it is the same amount of words are available in the extracted text file. In doing so words could occur in different places in a text but could still be classified as correct. This is presented in the example. The word 'advertisement' in the content file is part of the actual content. In the extracted text file, the sentence 'My content is about advertisement in modern times.' is not found but there is still a text passage classified as content where the word 'advertisement' occurs. As a result, the word advertisement would be classified as correct (True positive) with this first approach because it is counted once in each text. So this approach could not be used.
In a second approach, the comparison was done by not only comparing a single word but as well as the word before and after the word of interest. 
For the example above, for each word, a group of three words is built and this group is then compared with the second text. The word 'advertisement' would be defined as the word group 'about advertisement in' and the second text is searched for this pattern. Since there is not such a group, the word advertisement would be classified wrong (False positive) which would be correct. With this approach the results were already a lot better. But there were still situations where this approach was not working. Instead of implementing an elaborated text comparison algorithm we decided to fall back on existing solutions. Merging tools like diff \cite{wiki:diff} or meld \cite{wiki:meld} are doing exactly the task we were looking for. We decided to use the open source implementation merge-diff-patch \cite{google:diffMatchPatch} developed by google.  The outcome of the comparison performed by diff-match-patch are all the words of the input text grouped in 'EQUAL', 'DELETE' and 'INSERT' which can be easy transferred into the needed values True Positive ('EQUAL'), False Positive ('INSERT'), True Negative ('DELETE') and False Negative (remaining words). 


The next task after handling these risks was to integrate the prototypes into the main application. The remaining programming tasks were then refactoring the first approach and testing the application. The final software architecture is described in the abbreviation under Appendix \ref{architecture}. The test concept is described under Appendix \ref{Test plan}. 

The final programming task would have been the implementation of a new text extraction algorithm. On the MS4 meeting ( Appendix \ref{ms4report}), we decided not to implement an additional text extraction algorithm because we needed to investigate the problems of the existing algorithms so that we have an idea what we need to improve for our own algorithm. So instead of focusing on a new approach, we decided to focus on the task of improve the test framework. The improvements which were needed to investigate the results more close was a way that it is possible to filter the results for the test cases of interest and investigate single test cases more close. 

\subsection{Filtering results}
The first needed improvement was the possibility of filtering all the test cases for certain criterion. To do so, a new configuration item was defined which could be added to the configuration file before starting the test run. The filter function is explained with following example.

If one want to have all test cases in which JusText had a good rating for Precision and Boilerplate did not, following lines could be added to the configuration file.

\begin{lstlisting}
filter:justext:precision:min:0.8;
filter:boilerpipe:precision:max:0.2;
\end{lstlisting} 

Every test case which has precision value below 80\% for JusText and a precision value above 20\% for Boilerpipe was now removed from the results. The resulting test cases were all test cases where JusText performed quite well and Boilerpipe did not. These test cases could then investigated more closely.
With this new function it was now much easier to investigate certain test cases more closely.

\subsection{Evaluation based on blocks}

The second needed improvement was the possibility of evaluating a single test instead of getting the results for a summary of tests.
The most information about a single test such as Precision, Recall and the extracted text by the algorithms was already available. The more challenging part was to get the classification based on the bocks generated by the algorithms. Both Justext and Boilerpipe do not have an according API call where it is possible to get the output results including the classification information for the blocks. Which means that the two algorithms needed to be modified.

\subsubsection{Modification JusText}
\label{subsec:Modification JusText}

Even with no prior knowledge at the programming language python, the task was easier then expected. The algorithm is working with blocks during the whole process and prints the blocks classified as content. All we needed to do is to print the classification values at the beginning of each block and parse the output string accordingly with the main application.
This is an example of the printed string after my modification. \linebreak

\begin{lstlisting}
<p class="bad" cfclass="bad" heading="0" word\_count="3" link\_density="1" 
stopword\_count="1" stopword\_density="0"\> Kites with Antennas>
\end{lstlisting}

First, all the classification data like 'link density' and 'stop word count' was printed followd by the text of the actual block which in this example would be 'Kites with Antennas'. This string could then be parsed by the test framework for further use.


\subsubsection{Modification Boilerpipe}

Adapting the boilerpipe algorithm on the other hand was not as easy as expected. Boilerpipe does merge the single blocks during the algorithm and the classification information for some blocks are lost due to this approach. My solution was then to edit the algorithm that each block is backed before it is merged with others and all the available classification information as well. The problem with this approach is that some blocks are classified different at a later point of time and the backed data is not correct anymore. To solve this problem, bigger changes of the algorithm would be needed. However, this solution is good enough to evaluate the results accordingly.


\section{Results}


\subsection{Statistical data}

With the test framework it was finally possible to produce the classification data for the  algorithms using an HTML source file and a content file as input data. Doing this with a few test files, it is not possible to produce a concrete quality criterion for an algorithm. A bigger test data was needed. A gold standard test data was used for this purpose. This test data was used for a text extraction competition called CleanEval \cite{CleanEval:online}. 
The following table shows the overall results when extracting the CeanEval test files with JusText and Boilerpipe and process the results with the test framework.


\begin{tabular}{| p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} | p{2.2cm} |}
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall}  & \textbf{Fallout}  & \textbf{F-Measure}  &\textbf{Accuracy} \\ \hline
    JusText     & 95.29 \%       &  91.99 \%  & 35.03 \% & 91.37 \% & 85.11 \%   \\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%  & 49.31 \% & 79.56 \% &  68.17 \% \\ \hline

\end{tabular}

The meaning of the single result types is described in Appendix section \ref{subsec:Evaluation of classification}.



\subsection{Analysis}

This test data was not only used in this but as well in other papers which broach the issue of text extraction. The paper 'More Effective Boilerplate Removal—the GoldMiner Algorithm' \cite{paper:goldminer} used the same test data to compare their Goldminer algorithm to JusText and Boilerpipe. The results from this paper were used to determine if my approach is heading into the right direction or if the outcome is completely wrong. 
The results from the paper testing the different algorithms with the gold standard  are shown in table \ref{table:resultsGoldminer}.

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    JusText     & 95.29 \%       &  91.99 \%		\\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%		\\ \hline
\end{tabular}
\caption[asdfasdf]{The results for the CleanEval test set from the Goldminer paper}
\label{table:resultsGoldminer}
\end{table}

The results from my test framework are shown in table \ref{table:resultsComparison} 

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    JusText     & 95.86 \%       &  87.27 \%		\\ \hline
    Boilerpipe & 91.14 \%       &  70.60 \%		\\ \hline
\end{tabular}
\caption{The results for the CleanEval test set from the test framework}
\label{table:resultsComparison}
\end{table}


The results are not exaclty the same but they are close. The difference could be explained by several points. First, there is no guaranty that the results from the paper are correct. Second, most of the common approaches comparing these text extraction algorithms were done in comparing the correct classified HTML blocks and not the single words like I did it in this project. This can change the results significantly. Following example should clarify this statement.

Suppose we have an HTML document with ten blocks which have a certain amount of words and are classified by an algorithm as defined in table \ref{blockWordExample}.

\begin{table}[!ht]
\begin{tabular}{| p{2cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Block No.}      & \textbf{Word count}  & \textbf{Classification} 				\\ \hline
    1     & 10      	&  True Positive	\\ \hline
    2     & 100     	&  True Positive	\\ \hline
    3     & 50       	&  True positive	\\ \hline
    4     & 30       	&  True Positive	\\ \hline
    5     & 1       	&  False Positive	\\ \hline
    6     & 1000      	&  False Positive	\\ \hline
    7     & 20      	&  False Positive	\\ \hline
    8     & 300       	&  False Positive	\\ \hline
    9     & 200       	&  False Negative	\\ \hline
    10    & 50        	&  False Negative	\\ \hline
\end{tabular}
\caption{Example Blocks vs. Words}
\label{blockWordExample}
\end{table}

The formulas for calculating Precision and Recall are described in the appendix section \ref{subsec:Evaluation of classification}.
The calculated values for Precision and Recall from table \ref{blockWordExample} is shown in table \ref{ResultsBlockWordExample}

\begin{table}[!ht]
\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \ \textbf{Algorithm}  & \textbf{Calculation based on blocks}  & \textbf{Calculation based on words} 				\\ \hline
    Precision     & 50 \%    	&  12.57 \%	\\ \hline
    Recall & 66.67 \%    	&  25.68 \%	\\ \hline
\end{tabular}
\caption{Results example Blocks vs. Words}
\label{ResultsBlockWordExample}
\end{table}

We can see that the difference between the two approach is quite significant because a block which is classified wrong and contains a lot of words is not weighted as high if the values are calculated based on blocks instead of words. 
Comparing algorithms based on words seems to be the better approach since the results are more accurate. Furthermore, each algorithm can define the size of a block by itself and the two algorithms do not produce the same amount of blocks. Comparing a different amount of blocks does not produce very accurate results as well.  

\subsection{Detailed analysis}
\label{subsec:Detailed Analysis}

Beside getting this statistical results and compare the general performance of text extraction algorithms, it is possible to investigate single test cases more closely. To do so a single line with the test case name needs to be added to the configuration file.

\begin{lstlisting}
inspect:testOfInterest;
\end{lstlisting}

Now the test case 'testOfInterest' is inspected more closely and an extra output file is generated when a test run is performed. Appendix \ref{example:resultsSingleTest} shows an example of a detailed result file for a single test.

In the first part the statistical values are printed followed by the actual content text which is compared to the output text of the algorithms. Then the results of the tested algorithms are printed, first as continuous text followed by the results formatted as blocks with the according block classification.
The following example shows a single block classified by the algorithms.


\textbf{Boilerpipe}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 
NOT_DEFINED; text_Density: 3.0; context_Free_classification: NOT_DEFINED; ]
Kites with Antennas
\end{lstlisting}


\textbf{JusText}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 1; 
text_Density: NOT_DEFINED; context_Free_classification: CFC_BAD; ] 
Kites with Antennas
\end{lstlisting}

The classification values which can be extracted from the algorithms are:

\begin{itemize}
\item Link density,
\item Classification,
\item Word count,
\item Stop word count (only JusText),
\item Text density (Only Boilerpipe),
\item Context free classification (Only justext).
\end{itemize}

Having these values for a certain block already helps evaluating how the algorithms work and why a specific block is classified correct or wrong. Some classification values are only used by one algorithm. These values are marked as 'NOT\_DEFINED' in the output file for the according algorithm.
