% Chapter Template

\chapter{Solution Development} % Main chapter title

\label{Solution Development} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Solution Development}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the approach developing the application and the outcomes.

\section{Approach}

\subsection{Planning}

The planning of this project was a very straight forward task. Since it is a small one man project and the start and end dates were clearly defined there was not a lot of margins.  However I am working part time for Layzapp and this project was not the only task during the period of time and the project needed to be coordinated somehow with other tasks. My solution was to work about one to two weeks for on this project and switch to other tasks for the next one to two weeks. Switching between multiple projects within one week is not very effective because one needs always some hours to get back into the topic. 
A project plan was done for the whole time line of the project. Several milestones and its delivery objects were defined. As I was working with scrum a milestone was defined as sprint and the delivery objects were divided into stories on the start of each sprint. Since it was a one man team no weekly scrum meetings were. However on the end of each sprint there was a sprint meeting and the delivery objects were presented to the supervisors and the tasks for the next sprint were defined or adjusted. During the project, the tool toggl \footnote{see \ref{sec:Time tracking}  for more information about toggl} was used as a time tracker and task manager. The whole planning as well as the time tracking can be found in the abbreviations under \ref{Planning}.

\subsection{Programming}

As a first step a data model and a first approach of a software architecture was developed. This first approach was then implemented so that the basic functionality of the application is working. With this working version the data model and the software architecture could be approved. The next step was elimination the biggest risk which were the integration of the two algorithms Justext and Boilerpipe into the application. Prototypes for each algorithm were programmed. Implementing the Boilerpipe algorithm was an easy task since it is implemented in Java and could be used without the need of any modification. Implementing the Justext algorithm was a more difficult task since it is implemented in Python. The final solution was then to call the python application with system calls from the java application and read the output text file generated by the python program. 
The second big risk was the approach comparing the text with the actual content and the outcome of the algorithms and find the classification numbers (True positive, false positive, true negative, false negative). The different approaches are described on following example.

\textbf{Content file}

\texttt{I am an interesting text. My content is about advertisement.}

\textbf{Outcome algorithm}

\texttt{I am an interesting text. And i am advertisement about socks. }


In the example above, the algorithm classified the first sentence correct but classified another sentence from an advertisement block as content as well. If these text would be compared with the approche described above, the word "advertisement" would be defined as classified correctly (True positive) but it is not. 
In a second approach, the comparison was done by not only comparing a single word but as well as the word before and after the word of interest. 
For the example above, for each word, a group of three words is built and this group is then compared with the second text. The word "advertisement" would be defined as the word group "is about advertisement" and the second text was searched for this pattern. Since there is not such a group, the word advertisement would be defined as classified wrong (False positive). With this approach the results were already a lot better. But there were still situations where this approach was not working. Instead of implementing an elaborated text comparison algorithm I decided to fall back on existing solutions. Merging tools like diff \footnote{\url{http://en.wikipedia.org/wiki/Diff_utility}} or meld \footnote{\url{http://en.wikipedia.org/wiki/Meld_(software)}} are performing exaclty the task I was looking for. I decided to use the Java open source implementation merge-diff-patch \footnote{\url{https://code.google.com/p/google-diff-match-patch/}} from google.  The outcome of the comparison are all the words of the input text grouped in 'EQUAL', 'DELETE' and 'INSERT' which can be easy transferred into the needed values True Positive ('EQUAL'), False Positive ('INSERT'), True Negative ('DELETE') and False Negative (remaining words). 


The next task after handling these risks was to integrate the prototypes into the main application. The remaining programming tasks were then refactoring the first approach and testing the application. The final software architecture is described in the abbreviation under \ref{architecture}. The test concept is described under lkasjfasldöf. 

The final programming task was then adapting the application so that it is easier to 


\subsection{Analysis}

The analysis of the result was the most time consuming task beside the programming tasks. With the test framework it was finally possible to produce the classification data for the  algorithms using an HTML file and a content file as input data. Doing this with a few test files, it is not possible to produce a concrete quality criterion for a algorithm. A bigger test data was needed. The gold standard test data was used for this purpose. This test data was used for a text extraction competition called CLEANEVAL \footnote{The test data and more information about CLEANEVAL can be found under \url{http://cleaneval.sigwac.org.uk/}}. This test data was not only used in the competition but as well in other papers which broach the issue of text extraction. The paper 'More Effective Boilerplate Removal—the GoldMiner Algorithm' used the same test data to compare the Goldminer algorithm to Justext and Boilerpipe. The results from this paper were used to determine if my approach is heading into the right direction or if the outcome is completely wrong. 
The results from the paper testing the different algorithms with the gold standard  are

\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    Justext     & 95.29 \%       &  91.99 \%		\\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%		\\ \hline
\end{tabular}

The results from my test framework are

\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    Justext     & 95.86 \%       &  87.27 \%		\\ \hline
    Boilerpipe & 91.14 \%       &  70.60 \%		\\ \hline
\end{tabular}

The results are not exaclty the same but they are close. The difference could be explained by several points. First, there is no guaranty that the results from the paper are correct. Second most of the common approaches comparing these text extraction algorithms were done in comparing the correct classified HTML blocks and not the single words like I did it in this project. This can change the results significantly. Following example should clarify this statement.

Suppose we have a HTML document with ten blocks which have a certain amount of words and are classified by an algorithm as defined in following table.

\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \textbf{Block No.}      & \textbf{Word count}  & \textbf{Classification} 				\\ \hline
    1     & 10      	&  True Positive	\\ \hline
    2     & 100     	&  True Positive	\\ \hline
    3     & 50       	&  True positive	\\ \hline
    4     & 30       	&  True Positive	\\ \hline
    5     & 1       	&  False Positive	\\ \hline
    6     & 1000      	&  False Positive	\\ \hline
    7     & 20      	&  False Positive	\\ \hline
    8     & 300       	&  False Positive	\\ \hline
    9     & 200       	&  False Negative	\\ \hline
    10    & 50        	&  False Negative	\\ \hline
\end{tabular}

The calculated values for Precision and Recall would be as follow.

\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \     & \textbf{Calculation based on blocks}  & \textbf{Calculation based on words} 				\\ \hline
    Precision     & 50 \%    	&  12.57 \%	\\ \hline
    Recall & 66.67 \%    	&  25.68 \%	\\ \hline
\end{tabular}

We can see that the difference between the two approach is very big because a block which is classified wrong and contains a lot of words is not weighted as high if the values are calculated based on blocks instead of words. 
From my point of view comparing algorithms based on words is the better approach to compare the performance of text extraction algorithms since the results are more accurate. Furthermore, each algorithm can define the size of a block by itself and it is not defined that a HTML document extracted by two algorithms produce the same blocks. Comparing a different amount of blocks does not produce very accurate results as well.  



\section{Results}