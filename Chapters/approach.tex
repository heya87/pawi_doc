% Chapter Template

\chapter{Solution Development} % Main chapter title

\label{Solution Development} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{\emph{Solution Development}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter describes the approach developing the application and the outcomes.

\section{Approach}

\subsection{Planning}

The planning of this project was a very straight forward task. Since it is a small one man project and the start and end dates were clearly defined there was not a lot of margins.  However I am working part time for Layzapp and this project was not the only task during the period of time and the project needed to be coordinated somehow with other tasks. My solution was to work about one to two weeks for on this project and switch to other tasks for the next one to two weeks. Switching between multiple projects within one week is not very effective because one needs always some hours to get back into the topic. 
A project plan was done for the whole time line of the project. Several milestones and its delivery objects were defined. As I was working with scrum a milestone was defined as sprint and the delivery objects were divided into stories on the start of each sprint. Since it was a one man team no weekly scrum meetings were. However on the end of each sprint there was a sprint meeting and the delivery objects were presented to the supervisors and the tasks for the next sprint were defined or adjusted. During the project, the tool toggl \footnote{see \ref{sec:Time tracking}  for more information about toggl} was used as a time tracker and task manager. The whole planning as well as the time tracking can be found in the abbreviations under \ref{Planning}.

\subsection{Programming}

As a first step a data model and a first approach of a software architecture was developed. This first approach was then implemented so that the basic functionality of the application is working. With this working version the data model and the software architecture could be approved. The next step was elimination the biggest risk which were the integration of the two algorithms Justext and Boilerpipe into the application. Prototypes for each algorithm were programmed. Implementing the Boilerpipe algorithm was an easy task since it is implemented in Java and could be used without the need of any modification. Implementing the Justext algorithm was a more difficult task since it is implemented in Python. The final solution was then to call the python application with system calls from the java application and read the output text file generated by the python program. 
The second big risk was the approach comparing the text with the actual content and the outcome of the algorithms and find the classification numbers (True positive, false positive, true negative, false negative). The different approaches are described on following example.

\textbf{Content file}

\texttt{I am an interesting text. My content is about advertisement.}

\textbf{Outcome algorithm}

\texttt{I am an interesting text. And i am advertisement about socks. }


In the example above, the algorithm classified the first sentence correct but classified another sentence from an advertisement block as content as well. If these text would be compared with the approche described above, the word "advertisement" would be defined as classified correctly (True positive) but it is not. 
In a second approach, the comparison was done by not only comparing a single word but as well as the word before and after the word of interest. 
For the example above, for each word, a group of three words is built and this group is then compared with the second text. The word "advertisement" would be defined as the word group "is about advertisement" and the second text was searched for this pattern. Since there is not such a group, the word advertisement would be defined as classified wrong (False positive). With this approach the results were already a lot better. But there were still situations where this approach was not working. Instead of implementing an elaborated text comparison algorithm I decided to fall back on existing solutions. Merging tools like diff \footnote{\url{http://en.wikipedia.org/wiki/Diff\_utility}} or meld \footnote{\url{http://en.wikipedia.org/wiki/Meld\_(software)}} are performing exaclty the task I was looking for. I decided to use the Java open source implementation merge-diff-patch \footnote{\url{https://code.google.com/p/google-diff-match-patch/}} from google.  The outcome of the comparison are all the words of the input text grouped in 'EQUAL', 'DELETE' and 'INSERT' which can be easy transferred into the needed values True Positive ('EQUAL'), False Positive ('INSERT'), True Negative ('DELETE') and False Negative (remaining words). 


The next task after handling these risks was to integrate the prototypes into the main application. The remaining programming tasks were then refactoring the first approach and testing the application. The final software architecture is described in the abbreviation under \ref{architecture}. The test concept is described under lkasjfasldöf. 

The final programming task was then adapting the application so that it is possible to evaluate a single test instead of getting the results for a summary of tests. The reason doing this is described in the section \ref{subsec:Detailed Analysis}. 
The most information about a single test such as Precision, Recall and the extracted text by the algorithms was already available. The more challenging part was the output of the evaluation based on bocks. The problem is that both algorithm have no way of getting the classification of the single blocks. Which means that the two algorithms needed to be modified.

\subsection{Modification Justext}
\label{subsec:Modification Justext}

After getting used to the programming language python, the task was easier then expected. The algorithm is working with blocks during the whole process and prints the blocks classified as content. All I needed to do is to print the classification values at the beginning of each block and evaluate the outcome with the Java application.
This is an example of the printed string after my modification.

\begin{lstlisting}
<p class="bad" cfclass="bad" heading="0" word\_count="3" link\_density="1" 
stopword\_count="1" stopword\_density="0"\> Kites with Antennas>
\end{lstlisting}

This string could then be parsed by the Java application into block objects and all the classification data like link density and stop word count could be set for each block.


\subsection{Modification Boilerpipe}

Adapting the boilerpipe algorithm on the other hand was not as easy as expected. Boilerpipe does merge the single blocks during the algorithm and the classification information for some blocks are lost due to this approach. My solution was then to edit the algorithm that each block is backed before it is merged with others and all the available classification information as well. The problem with this approach is that some blocks are classified different at a later point of time and the backed data is not correct anymore. To solve this problem, bigger changes of the algorithm would be needed. However, this solution is good enough to evaluate the results accordingly.


\section{Results}


\subsection{Statistical data}

TODO: conclusion of the output data and some graphs -.-

\subsection{Analysis}

With the test framework it was finally possible to produce the classification data for the  algorithms using an HTML file and a content file as input data. Doing this with a few test files, it is not possible to produce a concrete quality criterion for a algorithm. A bigger test data was needed. The gold standard test data was used for this purpose. This test data was used for a text extraction competition called CLEANEVAL \footnote{The test data and more information about CLEANEVAL can be found under \url{http://cleaneval.sigwac.org.uk/}}. This test data was not only used in the competition but as well in other papers which broach the issue of text extraction. The paper 'More Effective Boilerplate Removal—the GoldMiner Algorithm' used the same test data to compare the Goldminer algorithm to Justext and Boilerpipe. The results from this paper were used to determine if my approach is heading into the right direction or if the outcome is completely wrong. 
The results from the paper testing the different algorithms with the gold standard  are

\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    Justext     & 95.29 \%       &  91.99 \%		\\ \hline
    Boilerpipe & 95.15 \%       &  74.38 \%		\\ \hline
\end{tabular}

The results from my test framework are

\begin{tabular}{| p{3cm} | p{3cm} | p{3cm} | }
    \hline
    \textbf{Algorithm}      & \textbf{Precision}  & \textbf{Recall} 				\\ \hline
    Justext     & 95.86 \%       &  87.27 \%		\\ \hline
    Boilerpipe & 91.14 \%       &  70.60 \%		\\ \hline
\end{tabular}

The results are not exaclty the same but they are close. The difference could be explained by several points. First, there is no guaranty that the results from the paper are correct. Second most of the common approaches comparing these text extraction algorithms were done in comparing the correct classified HTML blocks and not the single words like I did it in this project. This can change the results significantly. Following example should clarify this statement.

Suppose we have a HTML document with ten blocks which have a certain amount of words and are classified by an algorithm as defined in following table.

\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \textbf{Block No.}      & \textbf{Word count}  & \textbf{Classification} 				\\ \hline
    1     & 10      	&  True Positive	\\ \hline
    2     & 100     	&  True Positive	\\ \hline
    3     & 50       	&  True positive	\\ \hline
    4     & 30       	&  True Positive	\\ \hline
    5     & 1       	&  False Positive	\\ \hline
    6     & 1000      	&  False Positive	\\ \hline
    7     & 20      	&  False Positive	\\ \hline
    8     & 300       	&  False Positive	\\ \hline
    9     & 200       	&  False Negative	\\ \hline
    10    & 50        	&  False Negative	\\ \hline
\end{tabular}

The calculated values for Precision and Recall would be as follow.

\begin{tabular}{| p{3cm} | p{5cm} | p{5cm} | }
    \hline
    \     & \textbf{Calculation based on blocks}  & \textbf{Calculation based on words} 				\\ \hline
    Precision     & 50 \%    	&  12.57 \%	\\ \hline
    Recall & 66.67 \%    	&  25.68 \%	\\ \hline
\end{tabular}

We can see that the difference between the two approach is very big because a block which is classified wrong and contains a lot of words is not weighted as high if the values are calculated based on blocks instead of words. 
From my point of view comparing algorithms based on words is the better approach to compare the performance of text extraction algorithms since the results are more accurate. Furthermore, each algorithm can define the size of a block by itself and it is not defined that a HTML document extracted by two algorithms produce the same blocks. Comparing a different amount of blocks does not produce very accurate results as well.  

\subsection{Detailed Analysis}
\label{subsec:Detailed Analysis}

It is now possible to compare multiple algorithms with each other and get a general idea if the algorithms are working well or not. However it is not possible to evaluate the strengths and weaknesses of a single algorithm more close. But this is needed to implement a new algorithm or improve the existing ones. As decided on the MS4 meeting I am not implementing a new algorithm but extending the functions of the existing application so it is easier to find this strengths or weaknesses. The ways to do this is described in this section.

As described in the introduction, the algorithms split the HTML file into single blocks and classify these blocks based on several classification data. To find out, why a certain test went very bad with one and very good with the other algorithm, it would be helpful to get the information about the block classification. After modifying the implementation of the algorithms (\ref{subsec:Modification Justext}) it is possible to get this data for both Justext and Boilerpipe. This is an example from the results of one block for both Justext and Boilerpipe.


\textbf{Boilerpipe}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 
NOT_DEFINED; text_Density: 3.0; context_Free_classification: NOT_DEFINED; ]
Kites with Antennas
\end{lstlisting}


\textbf{Justext}
\begin{lstlisting}
[link_density: 1.0; classification: BOILEPLATE; word_count: 3; stop_Word_Count: 1; 
text_Density: NOT_DEFINED; context_Free_classification: CFC_BAD; ] 
Kites with Antennas
\end{lstlisting}







This are the classification values which can be extracted from the algorithms without modifying the algorithms too much and it already can help evaluating how the algorithms work and why a specific block is classified as content by one algorithm and classified as boilerplate by the other one. Some classification values are only used by one algorithms. Because of this some values in the example are shown as NOT\_DEFINED.
